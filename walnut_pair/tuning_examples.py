#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è°ƒå‚ç¤ºä¾‹ä»£ç 
æä¾›å‡ ç§å¸¸è§çš„ä¼˜åŒ–æ–¹æ¡ˆä¾›å‚è€ƒ
"""

import numpy as np
import cv2
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans


# ==================== æ–¹æ¡ˆ1ï¼šç‰¹å¾åŠ æƒ ====================
def build_weighted_tensor(walnut_features, walnut_id):
    """æ–¹æ¡ˆ1ï¼šä¸ºä¸åŒç‰¹å¾ç±»å‹æ·»åŠ æƒé‡"""
    features = walnut_features[walnut_id]
    
    size_features = np.array([features['height'], features['width'], features['belly']])
    color_features = features.get('color', np.zeros(54))
    contour_features = features.get('contour', np.zeros(14))
    texture_features = features.get('fingerprint', np.zeros(6 * 2048))
    
    # ğŸ”§ å¯è°ƒå‚æ•°ï¼šæƒé‡æ¯”ä¾‹ï¼ˆæ ¹æ®å®é™…æ•ˆæœè°ƒæ•´ï¼‰
    weights = {
        'size': 1.0,      # å°ºå¯¸æƒé‡
        'color': 2.0,     # é¢œè‰²æƒé‡ï¼ˆé‡è¦ï¼‰
        'contour': 1.5,   # è½®å»“æƒé‡
        'texture': 1.0    # çº¹ç†æƒé‡
    }
    
    tensor = np.concatenate([
        size_features * weights['size'],
        color_features * weights['color'],
        contour_features * weights['contour'],
        texture_features * weights['texture']
    ])
    return tensor


# ==================== æ–¹æ¡ˆ2ï¼šæ”¹è¿›çš„PCAé™ç»´ ====================
def improved_pca_reduction(X, pca_dim=256, scaler_type='standard'):
    """æ–¹æ¡ˆ2ï¼šæ”¹è¿›çš„PCAé™ç»´"""
    
    # ğŸ”§ å¯è°ƒå‚æ•°ï¼šæ ‡å‡†åŒ–æ–¹æ³•
    scalers = {
        'standard': StandardScaler(),   # Z-scoreï¼ˆå½“å‰ä½¿ç”¨ï¼‰
        'minmax': MinMaxScaler(),       # å½’ä¸€åŒ–åˆ°[0,1]
        'robust': RobustScaler()        # å¯¹å¼‚å¸¸å€¼é²æ£’
    }
    
    scaler = scalers.get(scaler_type, StandardScaler())
    X_scaled = scaler.fit_transform(X)
    
    # ğŸ”§ å¯è°ƒå‚æ•°ï¼šPCAæ±‚è§£å™¨
    pca = PCA(n_components=pca_dim, svd_solver='auto')
    # svd_solveré€‰é¡¹ï¼š
    # - 'auto': è‡ªåŠ¨é€‰æ‹©ï¼ˆå½“å‰ä½¿ç”¨ï¼‰
    # - 'full': å®Œæ•´SVDï¼ˆæœ€ç²¾ç¡®ï¼‰
    # - 'arpack': é€‚åˆå¤§çŸ©é˜µ
    # - 'randomized': è¿‘ä¼¼è§£ï¼Œé€‚åˆè¶…å¤§çŸ©é˜µ
    
    X_final = pca.fit_transform(X_scaled)
    
    print(f"ä¿ç•™çš„æ–¹å·®æ¯”ä¾‹: {pca.explained_variance_ratio_.sum():.2%}")
    print(f"å‰10ä¸ªä¸»æˆåˆ†è§£é‡Šçš„æ–¹å·®: {pca.explained_variance_ratio_[:10].sum():.2%}")
    
    return X_final, pca


# ==================== æ–¹æ¡ˆ3ï¼šå¤šç›¸ä¼¼åº¦åº¦é‡èåˆ ====================
def calculate_multi_similarity(vec1, vec2):
    """æ–¹æ¡ˆ3ï¼šèåˆå¤šç§ç›¸ä¼¼åº¦åº¦é‡"""
    
    # ä½™å¼¦ç›¸ä¼¼åº¦
    cosine_sim = np.dot(vec1, vec2) / (
        np.linalg.norm(vec1) * np.linalg.norm(vec2) + 1e-12)
    
    # æ¬§æ°è·ç¦»ç›¸ä¼¼åº¦
    euclidean_dist = np.linalg.norm(vec1 - vec2)
    euclidean_sim = 1 / (1 + euclidean_dist)
    
    # æ›¼å“ˆé¡¿è·ç¦»ç›¸ä¼¼åº¦
    manhattan_dist = np.sum(np.abs(vec1 - vec2))
    manhattan_sim = 1 / (1 + manhattan_dist)
    
    # ğŸ”§ å¯è°ƒå‚æ•°ï¼šèåˆæƒé‡
    weights = {
        'cosine': 0.5,
        'euclidean': 0.3,
        'manhattan': 0.2
    }
    
    final_similarity = (
        weights['cosine'] * cosine_sim +
        weights['euclidean'] * euclidean_sim +
        weights['manhattan'] * manhattan_sim
    )
    
    return final_similarity


# ==================== æ–¹æ¡ˆ4ï¼šèšç±»é¢„ç­›é€‰ ====================
def similarity_with_clustering(X_final, n_clusters=20):
    """æ–¹æ¡ˆ4ï¼šä½¿ç”¨èšç±»é¢„ç­›é€‰ï¼Œå‡å°‘è®¡ç®—é‡"""
    
    # ğŸ”§ å¯è°ƒå‚æ•°ï¼šèšç±»æ•°é‡
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(X_final)
    
    # åªåœ¨åŒä¸€ç°‡å†…çš„æ ·æœ¬ä¹‹é—´è®¡ç®—ç›¸ä¼¼åº¦
    pairs = []
    for i in range(len(X_final)):
        for j in range(i+1, len(X_final)):
            if cluster_labels[i] == cluster_labels[j]:
                similarity = np.dot(X_final[i], X_final[j]) / (
                    np.linalg.norm(X_final[i]) * np.linalg.norm(X_final[j]) + 1e-12)
                pairs.append((i, j, similarity))
    
    pairs.sort(key=lambda x: x[2], reverse=True)
    return pairs


# ==================== æ–¹æ¡ˆ5ï¼šè‡ªé€‚åº”è§’åº¦åŠ æƒ ====================
def adaptive_angle_weighting(angle_features_dict):
    """æ–¹æ¡ˆ5ï¼šæ ¹æ®è§’åº¦ç‰¹å¾è´¨é‡è‡ªé€‚åº”åŠ æƒ"""
    
    angles = ['B', 'D', 'F', 'L', 'R', 'T']
    angle_names = ['èƒŒé¢', 'åº•éƒ¨', 'æ­£é¢', 'å·¦ä¾§', 'å³ä¾§', 'é¡¶éƒ¨']
    
    # è®¡ç®—æ¯ä¸ªè§’åº¦ç‰¹å¾çš„"è´¨é‡"ï¼ˆæ–¹å·®è¶Šå¤§ï¼Œä¿¡æ¯é‡è¶Šå¤§ï¼‰
    angle_qualities = {}
    for angle in angles:
        if angle_features_dict[angle] is not None:
            # ä½¿ç”¨ç‰¹å¾çš„æ ‡å‡†å·®ä½œä¸ºè´¨é‡æŒ‡æ ‡
            quality = np.std(angle_features_dict[angle])
            angle_qualities[angle] = quality
        else:
            angle_qualities[angle] = 0
    
    # å½’ä¸€åŒ–è´¨é‡åˆ†æ•°
    total_quality = sum(angle_qualities.values())
    if total_quality > 0:
        angle_weights = {k: v/total_quality for k, v in angle_qualities.items()}
    else:
        angle_weights = {angle: 1/6 for angle in angles}
    
    print("è§’åº¦æƒé‡ï¼š")
    for angle, name in zip(angles, angle_names):
        print(f"  {name}({angle}): {angle_weights[angle]:.3f}")
    
    return angle_weights


# ==================== æ–¹æ¡ˆ6ï¼šé¢œè‰²ç‰¹å¾æ”¹è¿› ====================
def extract_improved_color_features(img, mask):
    """æ–¹æ¡ˆ6ï¼šæ”¹è¿›çš„é¢œè‰²ç‰¹å¾æå–"""
    
    # ğŸ”§ å¯è°ƒå‚æ•°ï¼šç›´æ–¹å›¾binsæ•°é‡
    hist_bins = 16  # åŸå€¼ï¼š8ï¼Œå¢åŠ ç»†èŠ‚ä½†è®¡ç®—é‡å¢å¤§
    
    # å¢å¼ºé¢œè‰²ç©ºé—´
    bgr_mean = np.mean(img[mask > 0], axis=0)
    hsv_mean = np.mean(cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[mask > 0], axis=0)
    lab_mean = np.mean(cv2.cvtColor(img, cv2.COLOR_BGR2LAB)[mask > 0], axis=0)
    
    # å¢åŠ Gaborçº¹ç†ç‰¹å¾
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    gabor_kernels = []
    for theta in [0, 45, 90, 135]:
        kernel = cv2.getGaborKernel((21, 21), 5, theta, 10, 0.5, 0, ktype=cv2.CV_32F)
        gabor_kernels.append(kernel)
    
    gabor_features = []
    for kernel in gabor_kernels:
        filtered = cv2.filter2D(gray, cv2.CV_8UC3, kernel)
        gabor_features.append(np.mean(filtered[mask > 0]))
        gabor_features.append(np.std(filtered[mask > 0]))
    
    # ç»„åˆæ‰€æœ‰é¢œè‰²ç‰¹å¾
    color_features = np.concatenate([
        bgr_mean / 255.0,
        hsv_mean,
        lab_mean,
        np.array(gabor_features) / 255.0
    ])
    
    return color_features


# ==================== æ–¹æ¡ˆ7ï¼šä¸¤é˜¶æ®µåŒ¹é… ====================
def two_stage_matching(X_final, topk=30, stage1_k=100):
    """æ–¹æ¡ˆ7ï¼šä¸¤é˜¶æ®µåŒ¹é…ç­–ç•¥"""
    
    # é˜¶æ®µ1ï¼šå¿«é€Ÿç²—ç­›ï¼ˆä½¿ç”¨éƒ¨åˆ†ç‰¹å¾ï¼‰
    # å‡è®¾å‰500ç»´æ˜¯å¿«é€Ÿç‰¹å¾ï¼ˆå°ºå¯¸+é¢œè‰²+è½®å»“ï¼‰
    X_fast = X_final[:, :min(500, X_final.shape[1])]
    
    fast_pairs = []
    for i in range(len(X_fast)):
        for j in range(i+1, len(X_fast)):
            similarity = np.dot(X_fast[i], X_fast[j]) / (
                np.linalg.norm(X_fast[i]) * np.linalg.norm(X_fast[j]) + 1e-12)
            fast_pairs.append((i, j, similarity))
    
    fast_pairs.sort(key=lambda x: x[2], reverse=True)
    candidate_indices = set()
    for i, j, _ in fast_pairs[:stage1_k]:
        candidate_indices.add(i)
        candidate_indices.add(j)
    
    # é˜¶æ®µ2ï¼šç²¾ç¡®åŒ¹é…ï¼ˆä½¿ç”¨å®Œæ•´ç‰¹å¾ï¼‰
    final_pairs = []
    candidate_list = list(candidate_indices)
    for i_idx, i in enumerate(candidate_list):
        for j_idx, j in enumerate(candidate_list):
            if j_idx > i_idx:
                similarity = np.dot(X_final[i], X_final[j]) / (
                    np.linalg.norm(X_final[i]) * np.linalg.norm(X_final[j]) + 1e-12)
                final_pairs.append((i, j, similarity))
    
    final_pairs.sort(key=lambda x: x[2], reverse=True)
    return final_pairs[:topk]


# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================
if __name__ == "__main__":
    print("=" * 60)
    print("æ–‡ç©æ ¸æ¡ƒé…å¯¹ - è°ƒå‚ç¤ºä¾‹ä»£ç ")
    print("=" * 60)
    
    # ç¤ºä¾‹1ï¼šæµ‹è¯•ä¸åŒçš„PCAç»´åº¦å¯¹ç‰¹å¾ä¿ç•™çš„å½±å“
    print("\nã€ç¤ºä¾‹1ã€‘PCAç»´åº¦åˆ†æ")
    print("-" * 60)
    np.random.seed(42)
    X = np.random.randn(100, 12359)  # æ¨¡æ‹Ÿç‰¹å¾çŸ©é˜µ
    
    for pca_dim in [64, 128, 256, 512]:
        _, pca = improved_pca_reduction(X, pca_dim=pca_dim)
        print(f"PCAç»´åº¦={pca_dim}: "
              f"ä¿ç•™æ–¹å·®={pca.explained_variance_ratio_.sum():.2%}")
    
    # ç¤ºä¾‹2ï¼šæµ‹è¯•èšç±»æ•°é‡å¯¹ç­›é€‰æ•ˆæœçš„å½±å“
    print("\nã€ç¤ºä¾‹2ã€‘èšç±»é¢„ç­›é€‰åˆ†æ")
    print("-" * 60)
    np.random.seed(42)
    X_final = improved_pca_reduction(X, pca_dim=256)[0]
    
    for n_clusters in [10, 20, 30, 50]:
        pairs = similarity_with_clustering(X_final, n_clusters=n_clusters)
        print(f"èšç±»æ•°é‡={n_clusters}: å€™é€‰å¯¹æ•°é‡={len(pairs)}")
    
    # ç¤ºä¾‹3ï¼šæµ‹è¯•è‡ªé€‚åº”è§’åº¦åŠ æƒ
    print("\nã€ç¤ºä¾‹3ã€‘è‡ªé€‚åº”è§’åº¦åŠ æƒ")
    print("-" * 60)
    angle_features = {
        'B': np.random.randn(2048),
        'D': np.random.randn(2048) * 2,  # æ–¹å·®æ›´å¤§çš„è§’åº¦
        'F': np.random.randn(2048) * 0.5,
        'L': np.random.randn(2048),
        'R': np.random.randn(2048),
        'T': np.random.randn(2048) * 1.5
    }
    weights = adaptive_angle_weighting(angle_features)
    
    print("\nå®Œæˆï¼")
    print("=" * 60)
    print("æç¤ºï¼šè¿™äº›ç¤ºä¾‹ä»£ç å¯ä»¥ç›´æ¥é›†æˆåˆ°main.pyä¸­ä½¿ç”¨")
    print("å»ºè®®æŒ‰ç…§PARAMETER_TUNING_GUIDE.mdçš„ä¼˜å…ˆçº§é¡ºåºé€ä¸€å°è¯•ä¼˜åŒ–")

